{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Necessary Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:/Users/Geekquad/rnn_data/anna.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking out the first 500 characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverything was in confusion in the Oblonskys' house. The wife had\\ndiscovered that the husband was carrying on an intrigue with a French\\ngirl, who had been a governess in their family, and she had announced to\\nher husband that she could not go on living in the same house with him.\\nThis position of affairs had now lasted three days, and not only the\\nhusband and wife themselves, but all the members of their f\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cells below I am creating a couple of dictionaries to convert the characters to and from integers. \n",
    "Encoding the characters as integers makes it easier to use as input in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Creating two dictonaries\n",
    "   1. int2char : which maps integers to characters\n",
    "   2. char2int : which maps charaters to integers\"\"\"\n",
    "\n",
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate((chars)))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "#ENCODING THE TEXT:\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see those same characters from above, encoded as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([77, 42, 11, 58, 19, 22, 30, 20, 38, 62, 62, 62, 56, 11, 58, 58, 35,\n",
       "       20, 41, 11, 24, 17, 15, 17, 22, 49, 20, 11, 30, 22, 20, 11, 15, 15,\n",
       "       20, 11, 15, 17, 45, 22, 55, 20, 22, 21, 22, 30, 35, 20, 18, 46, 42,\n",
       "       11, 58, 58, 35, 20, 41, 11, 24, 17, 15, 35, 20, 17, 49, 20, 18, 46,\n",
       "       42, 11, 58, 58, 35, 20, 17, 46, 20, 17, 19, 49, 20, 76,  7, 46, 62,\n",
       "        7, 11, 35, 50, 62, 62, 68, 21, 22, 30, 35, 19, 42, 17, 46])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in out char-RNN, our LSTM expects an input that is one-hot encoded meaning, that each character is converted into an integer (by our created dictionary), and then converted into a column vector where only it's corresponding integer index will have the value of 1 and the rest of the vector will be filled with 0's. \n",
    "Making a one_hot_encoding function to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    one_hot = np.zeros((arr.size, n_labels), dtype = np.float32)\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "test_seq = np.array([[3, 5, 1]])\n",
    "one_hot = one_hot_encode(test_seq, 8)\n",
    "\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making training mini-batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train on this data, we will create mini-batches for training of some desired number of sequence steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    batch_size_total = batch_size*seq_length\n",
    "    n_batches = len(arr)//batch_size_total\n",
    "    \n",
    "    arr = arr[:n_batches*batch_size_total]\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:,:-1], y[:,-1] = x[:,1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll make some data sets and we can check out what's going on as we batch data. Here I am going to use a batch size of 8 and 50 sequence steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 8, 50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x/n [[77 42 11 58 19 22 30 20 38 62]\n",
      " [49 76 46 20 19 42 11 19 20 11]\n",
      " [22 46 75 20 76 30 20 11 20 41]\n",
      " [49 20 19 42 22 20 53 42 17 22]\n",
      " [20 49 11  7 20 42 22 30 20 19]\n",
      " [53 18 49 49 17 76 46 20 11 46]\n",
      " [20 12 46 46 11 20 42 11 75 20]\n",
      " [31  8 15 76 46 49 45 35 50 20]]\n",
      "\n",
      "y\n",
      " [[42 11 58 19 22 30 20 38 62 62]\n",
      " [76 46 20 19 42 11 19 20 11 19]\n",
      " [46 75 20 76 30 20 11 20 41 76]\n",
      " [20 19 42 22 20 53 42 17 22 41]\n",
      " [49 11  7 20 42 22 30 20 19 22]\n",
      " [18 49 49 17 76 46 20 11 46 75]\n",
      " [12 46 46 11 20 42 11 75 20 49]\n",
      " [ 8 15 76 46 49 45 35 50 20 74]]\n"
     ]
    }
   ],
   "source": [
    "print('x/n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2, drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        out = self.dropout(r_output)\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(), weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "        seq_length: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    net.train()\n",
    "    opt = torch.optim.Adam(net.parameters(), lr = lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            \n",
    "            counter += 1\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            net.zero_grad()\n",
    "            \n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y                                         \n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                net.train()\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "n_hidden=512\n",
    "n_layers=2\n",
    "\n",
    "net = CharRNN(chars, n_hidden, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "seq_length = 100\n",
    "n_epochs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30... Step: 10... Loss: 1.3963... Val Loss: 1.3868\n",
      "Epoch: 1/30... Step: 20... Loss: 1.3998... Val Loss: 1.3824\n",
      "Epoch: 1/30... Step: 30... Loss: 1.3898... Val Loss: 1.3791\n",
      "Epoch: 1/30... Step: 40... Loss: 1.3694... Val Loss: 1.3806\n",
      "Epoch: 1/30... Step: 50... Loss: 1.3874... Val Loss: 1.3838\n",
      "Epoch: 1/30... Step: 60... Loss: 1.3171... Val Loss: 1.3796\n",
      "Epoch: 1/30... Step: 70... Loss: 1.3471... Val Loss: 1.3792\n",
      "Epoch: 1/30... Step: 80... Loss: 1.3371... Val Loss: 1.3775\n",
      "Epoch: 1/30... Step: 90... Loss: 1.3491... Val Loss: 1.3727\n",
      "Epoch: 1/30... Step: 100... Loss: 1.3522... Val Loss: 1.3731\n",
      "Epoch: 1/30... Step: 110... Loss: 1.3407... Val Loss: 1.3814\n",
      "Epoch: 1/30... Step: 120... Loss: 1.3172... Val Loss: 1.3738\n",
      "Epoch: 1/30... Step: 130... Loss: 1.3529... Val Loss: 1.3699\n",
      "Epoch: 2/30... Step: 140... Loss: 1.3880... Val Loss: 1.3799\n",
      "Epoch: 2/30... Step: 150... Loss: 1.3541... Val Loss: 1.3707\n",
      "Epoch: 2/30... Step: 160... Loss: 1.3623... Val Loss: 1.3667\n",
      "Epoch: 2/30... Step: 170... Loss: 1.3763... Val Loss: 1.3626\n",
      "Epoch: 2/30... Step: 180... Loss: 1.3220... Val Loss: 1.3655\n",
      "Epoch: 2/30... Step: 190... Loss: 1.3078... Val Loss: 1.3703\n",
      "Epoch: 2/30... Step: 200... Loss: 1.3082... Val Loss: 1.3713\n",
      "Epoch: 2/30... Step: 210... Loss: 1.3367... Val Loss: 1.3644\n",
      "Epoch: 2/30... Step: 220... Loss: 1.3238... Val Loss: 1.3622\n",
      "Epoch: 2/30... Step: 230... Loss: 1.3202... Val Loss: 1.3680\n",
      "Epoch: 2/30... Step: 240... Loss: 1.3516... Val Loss: 1.3624\n",
      "Epoch: 2/30... Step: 250... Loss: 1.3203... Val Loss: 1.3637\n",
      "Epoch: 2/30... Step: 260... Loss: 1.3037... Val Loss: 1.3635\n",
      "Epoch: 2/30... Step: 270... Loss: 1.3571... Val Loss: 1.3578\n",
      "Epoch: 3/30... Step: 280... Loss: 1.3170... Val Loss: 1.3695\n",
      "Epoch: 3/30... Step: 290... Loss: 1.3286... Val Loss: 1.3617\n",
      "Epoch: 3/30... Step: 300... Loss: 1.3086... Val Loss: 1.3550\n",
      "Epoch: 3/30... Step: 310... Loss: 1.3183... Val Loss: 1.3543\n",
      "Epoch: 3/30... Step: 320... Loss: 1.3004... Val Loss: 1.3576\n",
      "Epoch: 3/30... Step: 330... Loss: 1.3079... Val Loss: 1.3614\n",
      "Epoch: 3/30... Step: 340... Loss: 1.3338... Val Loss: 1.3546\n",
      "Epoch: 3/30... Step: 350... Loss: 1.3186... Val Loss: 1.3552\n",
      "Epoch: 3/30... Step: 360... Loss: 1.2836... Val Loss: 1.3571\n",
      "Epoch: 3/30... Step: 370... Loss: 1.3153... Val Loss: 1.3550\n",
      "Epoch: 3/30... Step: 380... Loss: 1.3357... Val Loss: 1.3558\n",
      "Epoch: 3/30... Step: 390... Loss: 1.3127... Val Loss: 1.3543\n",
      "Epoch: 3/30... Step: 400... Loss: 1.2967... Val Loss: 1.3525\n",
      "Epoch: 3/30... Step: 410... Loss: 1.3151... Val Loss: 1.3449\n",
      "Epoch: 4/30... Step: 420... Loss: 1.3083... Val Loss: 1.3512\n",
      "Epoch: 4/30... Step: 430... Loss: 1.3024... Val Loss: 1.3446\n",
      "Epoch: 4/30... Step: 440... Loss: 1.3317... Val Loss: 1.3399\n",
      "Epoch: 4/30... Step: 450... Loss: 1.2734... Val Loss: 1.3519\n",
      "Epoch: 4/30... Step: 460... Loss: 1.2663... Val Loss: 1.3516\n",
      "Epoch: 4/30... Step: 470... Loss: 1.3129... Val Loss: 1.3536\n",
      "Epoch: 4/30... Step: 480... Loss: 1.3303... Val Loss: 1.3460\n",
      "Epoch: 4/30... Step: 490... Loss: 1.3163... Val Loss: 1.3427\n",
      "Epoch: 4/30... Step: 500... Loss: 1.3284... Val Loss: 1.3460\n",
      "Epoch: 4/30... Step: 510... Loss: 1.3069... Val Loss: 1.3430\n",
      "Epoch: 4/30... Step: 520... Loss: 1.3118... Val Loss: 1.3418\n",
      "Epoch: 4/30... Step: 530... Loss: 1.2951... Val Loss: 1.3393\n",
      "Epoch: 4/30... Step: 540... Loss: 1.2665... Val Loss: 1.3394\n",
      "Epoch: 4/30... Step: 550... Loss: 1.3317... Val Loss: 1.3351\n",
      "Epoch: 5/30... Step: 560... Loss: 1.2903... Val Loss: 1.3424\n",
      "Epoch: 5/30... Step: 570... Loss: 1.2968... Val Loss: 1.3369\n",
      "Epoch: 5/30... Step: 580... Loss: 1.2881... Val Loss: 1.3290\n",
      "Epoch: 5/30... Step: 590... Loss: 1.2860... Val Loss: 1.3420\n",
      "Epoch: 5/30... Step: 600... Loss: 1.2760... Val Loss: 1.3370\n",
      "Epoch: 5/30... Step: 610... Loss: 1.2634... Val Loss: 1.3391\n",
      "Epoch: 5/30... Step: 620... Loss: 1.2823... Val Loss: 1.3301\n",
      "Epoch: 5/30... Step: 630... Loss: 1.3067... Val Loss: 1.3335\n",
      "Epoch: 5/30... Step: 640... Loss: 1.2697... Val Loss: 1.3336\n",
      "Epoch: 5/30... Step: 650... Loss: 1.2888... Val Loss: 1.3333\n",
      "Epoch: 5/30... Step: 660... Loss: 1.2706... Val Loss: 1.3314\n",
      "Epoch: 5/30... Step: 670... Loss: 1.2824... Val Loss: 1.3297\n",
      "Epoch: 5/30... Step: 680... Loss: 1.2887... Val Loss: 1.3274\n",
      "Epoch: 5/30... Step: 690... Loss: 1.2898... Val Loss: 1.3302\n",
      "Epoch: 6/30... Step: 700... Loss: 1.2924... Val Loss: 1.3374\n",
      "Epoch: 6/30... Step: 710... Loss: 1.2756... Val Loss: 1.3300\n",
      "Epoch: 6/30... Step: 720... Loss: 1.2664... Val Loss: 1.3227\n",
      "Epoch: 6/30... Step: 730... Loss: 1.2876... Val Loss: 1.3328\n",
      "Epoch: 6/30... Step: 740... Loss: 1.2648... Val Loss: 1.3313\n",
      "Epoch: 6/30... Step: 750... Loss: 1.2635... Val Loss: 1.3318\n",
      "Epoch: 6/30... Step: 760... Loss: 1.2963... Val Loss: 1.3245\n",
      "Epoch: 6/30... Step: 770... Loss: 1.2772... Val Loss: 1.3314\n",
      "Epoch: 6/30... Step: 780... Loss: 1.2726... Val Loss: 1.3291\n",
      "Epoch: 6/30... Step: 790... Loss: 1.2678... Val Loss: 1.3287\n",
      "Epoch: 6/30... Step: 800... Loss: 1.2841... Val Loss: 1.3242\n",
      "Epoch: 6/30... Step: 810... Loss: 1.2560... Val Loss: 1.3229\n",
      "Epoch: 6/30... Step: 820... Loss: 1.2295... Val Loss: 1.3195\n",
      "Epoch: 6/30... Step: 830... Loss: 1.2800... Val Loss: 1.3251\n",
      "Epoch: 7/30... Step: 840... Loss: 1.2552... Val Loss: 1.3249\n",
      "Epoch: 7/30... Step: 850... Loss: 1.2614... Val Loss: 1.3197\n",
      "Epoch: 7/30... Step: 860... Loss: 1.2511... Val Loss: 1.3204\n",
      "Epoch: 7/30... Step: 870... Loss: 1.2564... Val Loss: 1.3229\n",
      "Epoch: 7/30... Step: 880... Loss: 1.2696... Val Loss: 1.3211\n",
      "Epoch: 7/30... Step: 890... Loss: 1.2681... Val Loss: 1.3206\n",
      "Epoch: 7/30... Step: 900... Loss: 1.2685... Val Loss: 1.3196\n",
      "Epoch: 7/30... Step: 910... Loss: 1.2382... Val Loss: 1.3222\n",
      "Epoch: 7/30... Step: 920... Loss: 1.2614... Val Loss: 1.3210\n",
      "Epoch: 7/30... Step: 930... Loss: 1.2474... Val Loss: 1.3166\n",
      "Epoch: 7/30... Step: 940... Loss: 1.2452... Val Loss: 1.3218\n",
      "Epoch: 7/30... Step: 950... Loss: 1.2634... Val Loss: 1.3189\n",
      "Epoch: 7/30... Step: 960... Loss: 1.2701... Val Loss: 1.3163\n",
      "Epoch: 7/30... Step: 970... Loss: 1.2770... Val Loss: 1.3198\n",
      "Epoch: 8/30... Step: 980... Loss: 1.2449... Val Loss: 1.3130\n",
      "Epoch: 8/30... Step: 990... Loss: 1.2469... Val Loss: 1.3181\n",
      "Epoch: 8/30... Step: 1000... Loss: 1.2445... Val Loss: 1.3134\n",
      "Epoch: 8/30... Step: 1010... Loss: 1.2769... Val Loss: 1.3138\n",
      "Epoch: 8/30... Step: 1020... Loss: 1.2724... Val Loss: 1.3169\n",
      "Epoch: 8/30... Step: 1030... Loss: 1.2502... Val Loss: 1.3138\n",
      "Epoch: 8/30... Step: 1040... Loss: 1.2525... Val Loss: 1.3137\n",
      "Epoch: 8/30... Step: 1050... Loss: 1.2438... Val Loss: 1.3150\n",
      "Epoch: 8/30... Step: 1060... Loss: 1.2423... Val Loss: 1.3158\n",
      "Epoch: 8/30... Step: 1070... Loss: 1.2502... Val Loss: 1.3169\n",
      "Epoch: 8/30... Step: 1080... Loss: 1.2439... Val Loss: 1.3141\n",
      "Epoch: 8/30... Step: 1090... Loss: 1.2471... Val Loss: 1.3121\n",
      "Epoch: 8/30... Step: 1100... Loss: 1.2296... Val Loss: 1.3115\n",
      "Epoch: 8/30... Step: 1110... Loss: 1.2414... Val Loss: 1.3090\n",
      "Epoch: 9/30... Step: 1120... Loss: 1.2378... Val Loss: 1.3107\n",
      "Epoch: 9/30... Step: 1130... Loss: 1.2607... Val Loss: 1.3130\n",
      "Epoch: 9/30... Step: 1140... Loss: 1.2553... Val Loss: 1.3060\n",
      "Epoch: 9/30... Step: 1150... Loss: 1.2736... Val Loss: 1.3107\n",
      "Epoch: 9/30... Step: 1160... Loss: 1.2338... Val Loss: 1.3124\n",
      "Epoch: 9/30... Step: 1170... Loss: 1.2480... Val Loss: 1.3049\n",
      "Epoch: 9/30... Step: 1180... Loss: 1.2316... Val Loss: 1.3047\n",
      "Epoch: 9/30... Step: 1190... Loss: 1.2676... Val Loss: 1.3104\n",
      "Epoch: 9/30... Step: 1200... Loss: 1.2258... Val Loss: 1.3028\n",
      "Epoch: 9/30... Step: 1210... Loss: 1.2253... Val Loss: 1.3066\n",
      "Epoch: 9/30... Step: 1220... Loss: 1.2334... Val Loss: 1.3082\n",
      "Epoch: 9/30... Step: 1230... Loss: 1.2247... Val Loss: 1.3060\n",
      "Epoch: 9/30... Step: 1240... Loss: 1.2250... Val Loss: 1.3005\n",
      "Epoch: 9/30... Step: 1250... Loss: 1.2401... Val Loss: 1.3004\n",
      "Epoch: 10/30... Step: 1260... Loss: 1.2348... Val Loss: 1.3018\n",
      "Epoch: 10/30... Step: 1270... Loss: 1.2394... Val Loss: 1.2994\n",
      "Epoch: 10/30... Step: 1280... Loss: 1.2564... Val Loss: 1.2920\n",
      "Epoch: 10/30... Step: 1290... Loss: 1.2399... Val Loss: 1.2990\n",
      "Epoch: 10/30... Step: 1300... Loss: 1.2269... Val Loss: 1.2975\n",
      "Epoch: 10/30... Step: 1310... Loss: 1.2429... Val Loss: 1.2928\n",
      "Epoch: 10/30... Step: 1320... Loss: 1.2101... Val Loss: 1.2958\n",
      "Epoch: 10/30... Step: 1330... Loss: 1.2080... Val Loss: 1.2978\n",
      "Epoch: 10/30... Step: 1340... Loss: 1.2088... Val Loss: 1.2960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/30... Step: 1350... Loss: 1.2125... Val Loss: 1.2984\n",
      "Epoch: 10/30... Step: 1360... Loss: 1.2118... Val Loss: 1.2936\n",
      "Epoch: 10/30... Step: 1370... Loss: 1.2100... Val Loss: 1.2877\n",
      "Epoch: 10/30... Step: 1380... Loss: 1.2391... Val Loss: 1.2908\n",
      "Epoch: 10/30... Step: 1390... Loss: 1.2673... Val Loss: 1.2948\n",
      "Epoch: 11/30... Step: 1400... Loss: 1.2420... Val Loss: 1.2898\n",
      "Epoch: 11/30... Step: 1410... Loss: 1.2614... Val Loss: 1.2909\n",
      "Epoch: 11/30... Step: 1420... Loss: 1.2530... Val Loss: 1.2855\n",
      "Epoch: 11/30... Step: 1430... Loss: 1.2212... Val Loss: 1.2918\n",
      "Epoch: 11/30... Step: 1440... Loss: 1.2392... Val Loss: 1.2917\n",
      "Epoch: 11/30... Step: 1450... Loss: 1.1737... Val Loss: 1.2842\n",
      "Epoch: 11/30... Step: 1460... Loss: 1.2027... Val Loss: 1.2817\n",
      "Epoch: 11/30... Step: 1470... Loss: 1.1969... Val Loss: 1.2854\n",
      "Epoch: 11/30... Step: 1480... Loss: 1.2168... Val Loss: 1.2848\n",
      "Epoch: 11/30... Step: 1490... Loss: 1.2057... Val Loss: 1.2880\n",
      "Epoch: 11/30... Step: 1500... Loss: 1.1965... Val Loss: 1.2947\n",
      "Epoch: 11/30... Step: 1510... Loss: 1.1774... Val Loss: 1.2846\n",
      "Epoch: 11/30... Step: 1520... Loss: 1.2140... Val Loss: 1.2851\n",
      "Epoch: 12/30... Step: 1530... Loss: 1.2681... Val Loss: 1.2824\n",
      "Epoch: 12/30... Step: 1540... Loss: 1.2132... Val Loss: 1.2864\n",
      "Epoch: 12/30... Step: 1550... Loss: 1.2280... Val Loss: 1.2821\n",
      "Epoch: 12/30... Step: 1560... Loss: 1.2420... Val Loss: 1.2739\n",
      "Epoch: 12/30... Step: 1570... Loss: 1.1829... Val Loss: 1.2809\n",
      "Epoch: 12/30... Step: 1580... Loss: 1.1777... Val Loss: 1.2861\n",
      "Epoch: 12/30... Step: 1590... Loss: 1.1759... Val Loss: 1.2827\n",
      "Epoch: 12/30... Step: 1600... Loss: 1.2005... Val Loss: 1.2855\n",
      "Epoch: 12/30... Step: 1610... Loss: 1.1834... Val Loss: 1.2895\n",
      "Epoch: 12/30... Step: 1620... Loss: 1.1862... Val Loss: 1.2819\n",
      "Epoch: 12/30... Step: 1630... Loss: 1.2151... Val Loss: 1.2853\n",
      "Epoch: 12/30... Step: 1640... Loss: 1.1844... Val Loss: 1.2888\n",
      "Epoch: 12/30... Step: 1650... Loss: 1.1672... Val Loss: 1.2785\n",
      "Epoch: 12/30... Step: 1660... Loss: 1.2224... Val Loss: 1.2787\n",
      "Epoch: 13/30... Step: 1670... Loss: 1.1909... Val Loss: 1.2798\n",
      "Epoch: 13/30... Step: 1680... Loss: 1.1981... Val Loss: 1.2791\n",
      "Epoch: 13/30... Step: 1690... Loss: 1.1865... Val Loss: 1.2800\n",
      "Epoch: 13/30... Step: 1700... Loss: 1.1777... Val Loss: 1.2690\n",
      "Epoch: 13/30... Step: 1710... Loss: 1.1664... Val Loss: 1.2770\n",
      "Epoch: 13/30... Step: 1720... Loss: 1.1741... Val Loss: 1.2858\n",
      "Epoch: 13/30... Step: 1730... Loss: 1.2062... Val Loss: 1.2775\n",
      "Epoch: 13/30... Step: 1740... Loss: 1.1818... Val Loss: 1.2749\n",
      "Epoch: 13/30... Step: 1750... Loss: 1.1541... Val Loss: 1.2780\n",
      "Epoch: 13/30... Step: 1760... Loss: 1.1707... Val Loss: 1.2781\n",
      "Epoch: 13/30... Step: 1770... Loss: 1.2003... Val Loss: 1.2808\n",
      "Epoch: 13/30... Step: 1780... Loss: 1.1715... Val Loss: 1.2868\n",
      "Epoch: 13/30... Step: 1790... Loss: 1.1633... Val Loss: 1.2742\n",
      "Epoch: 13/30... Step: 1800... Loss: 1.1923... Val Loss: 1.2741\n",
      "Epoch: 14/30... Step: 1810... Loss: 1.1958... Val Loss: 1.2769\n",
      "Epoch: 14/30... Step: 1820... Loss: 1.1737... Val Loss: 1.2746\n",
      "Epoch: 14/30... Step: 1830... Loss: 1.1988... Val Loss: 1.2724\n",
      "Epoch: 14/30... Step: 1840... Loss: 1.1474... Val Loss: 1.2756\n",
      "Epoch: 14/30... Step: 1850... Loss: 1.1441... Val Loss: 1.2752\n",
      "Epoch: 14/30... Step: 1860... Loss: 1.1905... Val Loss: 1.2792\n",
      "Epoch: 14/30... Step: 1870... Loss: 1.1963... Val Loss: 1.2751\n",
      "Epoch: 14/30... Step: 1880... Loss: 1.1838... Val Loss: 1.2735\n",
      "Epoch: 14/30... Step: 1890... Loss: 1.2073... Val Loss: 1.2814\n",
      "Epoch: 14/30... Step: 1900... Loss: 1.1847... Val Loss: 1.2785\n",
      "Epoch: 14/30... Step: 1910... Loss: 1.1772... Val Loss: 1.2808\n",
      "Epoch: 14/30... Step: 1920... Loss: 1.1791... Val Loss: 1.2789\n",
      "Epoch: 14/30... Step: 1930... Loss: 1.1505... Val Loss: 1.2726\n",
      "Epoch: 14/30... Step: 1940... Loss: 1.2029... Val Loss: 1.2697\n",
      "Epoch: 15/30... Step: 1950... Loss: 1.1732... Val Loss: 1.2736\n",
      "Epoch: 15/30... Step: 1960... Loss: 1.1737... Val Loss: 1.2733\n",
      "Epoch: 15/30... Step: 1970... Loss: 1.1707... Val Loss: 1.2653\n",
      "Epoch: 15/30... Step: 1980... Loss: 1.1652... Val Loss: 1.2707\n",
      "Epoch: 15/30... Step: 1990... Loss: 1.1656... Val Loss: 1.2720\n",
      "Epoch: 15/30... Step: 2000... Loss: 1.1574... Val Loss: 1.2756\n",
      "Epoch: 15/30... Step: 2010... Loss: 1.1715... Val Loss: 1.2707\n",
      "Epoch: 15/30... Step: 2020... Loss: 1.1785... Val Loss: 1.2713\n",
      "Epoch: 15/30... Step: 2030... Loss: 1.1536... Val Loss: 1.2757\n",
      "Epoch: 15/30... Step: 2040... Loss: 1.1700... Val Loss: 1.2742\n",
      "Epoch: 15/30... Step: 2050... Loss: 1.1538... Val Loss: 1.2748\n",
      "Epoch: 15/30... Step: 2060... Loss: 1.1658... Val Loss: 1.2764\n",
      "Epoch: 15/30... Step: 2070... Loss: 1.1781... Val Loss: 1.2733\n",
      "Epoch: 15/30... Step: 2080... Loss: 1.1775... Val Loss: 1.2663\n",
      "Epoch: 16/30... Step: 2090... Loss: 1.1785... Val Loss: 1.2680\n",
      "Epoch: 16/30... Step: 2100... Loss: 1.1725... Val Loss: 1.2656\n",
      "Epoch: 16/30... Step: 2110... Loss: 1.1562... Val Loss: 1.2648\n",
      "Epoch: 16/30... Step: 2120... Loss: 1.1690... Val Loss: 1.2663\n",
      "Epoch: 16/30... Step: 2130... Loss: 1.1504... Val Loss: 1.2721\n",
      "Epoch: 16/30... Step: 2140... Loss: 1.1562... Val Loss: 1.2710\n",
      "Epoch: 16/30... Step: 2150... Loss: 1.1815... Val Loss: 1.2680\n",
      "Epoch: 16/30... Step: 2160... Loss: 1.1520... Val Loss: 1.2710\n",
      "Epoch: 16/30... Step: 2170... Loss: 1.1670... Val Loss: 1.2762\n",
      "Epoch: 16/30... Step: 2180... Loss: 1.1624... Val Loss: 1.2725\n",
      "Epoch: 16/30... Step: 2190... Loss: 1.1714... Val Loss: 1.2798\n",
      "Epoch: 16/30... Step: 2200... Loss: 1.1504... Val Loss: 1.2714\n",
      "Epoch: 16/30... Step: 2210... Loss: 1.1357... Val Loss: 1.2679\n",
      "Epoch: 16/30... Step: 2220... Loss: 1.1661... Val Loss: 1.2665\n",
      "Epoch: 17/30... Step: 2230... Loss: 1.1481... Val Loss: 1.2693\n",
      "Epoch: 17/30... Step: 2240... Loss: 1.1518... Val Loss: 1.2613\n",
      "Epoch: 17/30... Step: 2250... Loss: 1.1507... Val Loss: 1.2662\n",
      "Epoch: 17/30... Step: 2260... Loss: 1.1532... Val Loss: 1.2643\n",
      "Epoch: 17/30... Step: 2270... Loss: 1.1492... Val Loss: 1.2700\n",
      "Epoch: 17/30... Step: 2280... Loss: 1.1655... Val Loss: 1.2745\n",
      "Epoch: 17/30... Step: 2290... Loss: 1.1615... Val Loss: 1.2672\n",
      "Epoch: 17/30... Step: 2300... Loss: 1.1273... Val Loss: 1.2724\n",
      "Epoch: 17/30... Step: 2310... Loss: 1.1572... Val Loss: 1.2692\n",
      "Epoch: 17/30... Step: 2320... Loss: 1.1386... Val Loss: 1.2726\n",
      "Epoch: 17/30... Step: 2330... Loss: 1.1466... Val Loss: 1.2691\n",
      "Epoch: 17/30... Step: 2340... Loss: 1.1587... Val Loss: 1.2659\n",
      "Epoch: 17/30... Step: 2350... Loss: 1.1566... Val Loss: 1.2643\n",
      "Epoch: 17/30... Step: 2360... Loss: 1.1601... Val Loss: 1.2645\n",
      "Epoch: 18/30... Step: 2370... Loss: 1.1390... Val Loss: 1.2690\n",
      "Epoch: 18/30... Step: 2380... Loss: 1.1485... Val Loss: 1.2626\n",
      "Epoch: 18/30... Step: 2390... Loss: 1.1423... Val Loss: 1.2677\n",
      "Epoch: 18/30... Step: 2400... Loss: 1.1650... Val Loss: 1.2651\n",
      "Epoch: 18/30... Step: 2410... Loss: 1.1641... Val Loss: 1.2635\n",
      "Epoch: 18/30... Step: 2420... Loss: 1.1420... Val Loss: 1.2681\n",
      "Epoch: 18/30... Step: 2430... Loss: 1.1548... Val Loss: 1.2621\n",
      "Epoch: 18/30... Step: 2440... Loss: 1.1471... Val Loss: 1.2653\n",
      "Epoch: 18/30... Step: 2450... Loss: 1.1289... Val Loss: 1.2683\n",
      "Epoch: 18/30... Step: 2460... Loss: 1.1632... Val Loss: 1.2690\n",
      "Epoch: 18/30... Step: 2470... Loss: 1.1462... Val Loss: 1.2715\n",
      "Epoch: 18/30... Step: 2480... Loss: 1.1336... Val Loss: 1.2687\n",
      "Epoch: 18/30... Step: 2490... Loss: 1.1236... Val Loss: 1.2633\n",
      "Epoch: 18/30... Step: 2500... Loss: 1.1324... Val Loss: 1.2692\n",
      "Epoch: 19/30... Step: 2510... Loss: 1.1359... Val Loss: 1.2682\n",
      "Epoch: 19/30... Step: 2520... Loss: 1.1669... Val Loss: 1.2616\n",
      "Epoch: 19/30... Step: 2530... Loss: 1.1656... Val Loss: 1.2649\n",
      "Epoch: 19/30... Step: 2540... Loss: 1.1682... Val Loss: 1.2607\n",
      "Epoch: 19/30... Step: 2550... Loss: 1.1321... Val Loss: 1.2602\n",
      "Epoch: 19/30... Step: 2560... Loss: 1.1495... Val Loss: 1.2704\n",
      "Epoch: 19/30... Step: 2570... Loss: 1.1318... Val Loss: 1.2627\n",
      "Epoch: 19/30... Step: 2580... Loss: 1.1592... Val Loss: 1.2615\n",
      "Epoch: 19/30... Step: 2590... Loss: 1.1271... Val Loss: 1.2637\n",
      "Epoch: 19/30... Step: 2600... Loss: 1.1310... Val Loss: 1.2725\n",
      "Epoch: 19/30... Step: 2610... Loss: 1.1370... Val Loss: 1.2667\n",
      "Epoch: 19/30... Step: 2620... Loss: 1.1213... Val Loss: 1.2680\n",
      "Epoch: 19/30... Step: 2630... Loss: 1.1212... Val Loss: 1.2640\n",
      "Epoch: 19/30... Step: 2640... Loss: 1.1507... Val Loss: 1.2624\n",
      "Epoch: 20/30... Step: 2650... Loss: 1.1481... Val Loss: 1.2640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/30... Step: 2660... Loss: 1.1596... Val Loss: 1.2599\n",
      "Epoch: 20/30... Step: 2670... Loss: 1.1559... Val Loss: 1.2637\n",
      "Epoch: 20/30... Step: 2680... Loss: 1.1457... Val Loss: 1.2636\n",
      "Epoch: 20/30... Step: 2690... Loss: 1.1411... Val Loss: 1.2653\n",
      "Epoch: 20/30... Step: 2700... Loss: 1.1488... Val Loss: 1.2627\n",
      "Epoch: 20/30... Step: 2710... Loss: 1.1145... Val Loss: 1.2610\n",
      "Epoch: 20/30... Step: 2720... Loss: 1.1241... Val Loss: 1.2607\n",
      "Epoch: 20/30... Step: 2730... Loss: 1.1121... Val Loss: 1.2589\n",
      "Epoch: 20/30... Step: 2740... Loss: 1.1137... Val Loss: 1.2647\n",
      "Epoch: 20/30... Step: 2750... Loss: 1.1182... Val Loss: 1.2704\n",
      "Epoch: 20/30... Step: 2760... Loss: 1.1123... Val Loss: 1.2665\n",
      "Epoch: 20/30... Step: 2770... Loss: 1.1452... Val Loss: 1.2640\n",
      "Epoch: 20/30... Step: 2780... Loss: 1.1876... Val Loss: 1.2623\n",
      "Epoch: 21/30... Step: 2790... Loss: 1.1581... Val Loss: 1.2637\n",
      "Epoch: 21/30... Step: 2800... Loss: 1.1809... Val Loss: 1.2586\n",
      "Epoch: 21/30... Step: 2810... Loss: 1.1721... Val Loss: 1.2589\n",
      "Epoch: 21/30... Step: 2820... Loss: 1.1444... Val Loss: 1.2606\n",
      "Epoch: 21/30... Step: 2830... Loss: 1.1532... Val Loss: 1.2562\n",
      "Epoch: 21/30... Step: 2840... Loss: 1.0966... Val Loss: 1.2610\n",
      "Epoch: 21/30... Step: 2850... Loss: 1.1243... Val Loss: 1.2626\n",
      "Epoch: 21/30... Step: 2860... Loss: 1.1141... Val Loss: 1.2595\n",
      "Epoch: 21/30... Step: 2870... Loss: 1.1426... Val Loss: 1.2588\n",
      "Epoch: 21/30... Step: 2880... Loss: 1.1224... Val Loss: 1.2634\n",
      "Epoch: 21/30... Step: 2890... Loss: 1.1155... Val Loss: 1.2649\n",
      "Epoch: 21/30... Step: 2900... Loss: 1.0940... Val Loss: 1.2645\n",
      "Epoch: 21/30... Step: 2910... Loss: 1.1369... Val Loss: 1.2613\n",
      "Epoch: 22/30... Step: 2920... Loss: 1.2044... Val Loss: 1.2557\n",
      "Epoch: 22/30... Step: 2930... Loss: 1.1441... Val Loss: 1.2584\n",
      "Epoch: 22/30... Step: 2940... Loss: 1.1467... Val Loss: 1.2540\n",
      "Epoch: 22/30... Step: 2950... Loss: 1.1586... Val Loss: 1.2594\n",
      "Epoch: 22/30... Step: 2960... Loss: 1.1059... Val Loss: 1.2632\n",
      "Epoch: 22/30... Step: 2970... Loss: 1.1040... Val Loss: 1.2585\n",
      "Epoch: 22/30... Step: 2980... Loss: 1.1088... Val Loss: 1.2633\n",
      "Epoch: 22/30... Step: 2990... Loss: 1.1246... Val Loss: 1.2602\n",
      "Epoch: 22/30... Step: 3000... Loss: 1.1155... Val Loss: 1.2590\n",
      "Epoch: 22/30... Step: 3010... Loss: 1.1188... Val Loss: 1.2550\n",
      "Epoch: 22/30... Step: 3020... Loss: 1.1317... Val Loss: 1.2610\n",
      "Epoch: 22/30... Step: 3030... Loss: 1.1166... Val Loss: 1.2663\n",
      "Epoch: 22/30... Step: 3040... Loss: 1.0875... Val Loss: 1.2671\n",
      "Epoch: 22/30... Step: 3050... Loss: 1.1433... Val Loss: 1.2598\n",
      "Epoch: 23/30... Step: 3060... Loss: 1.1211... Val Loss: 1.2602\n",
      "Epoch: 23/30... Step: 3070... Loss: 1.1276... Val Loss: 1.2562\n",
      "Epoch: 23/30... Step: 3080... Loss: 1.1144... Val Loss: 1.2532\n",
      "Epoch: 23/30... Step: 3090... Loss: 1.1069... Val Loss: 1.2597\n",
      "Epoch: 23/30... Step: 3100... Loss: 1.1016... Val Loss: 1.2710\n",
      "Epoch: 23/30... Step: 3110... Loss: 1.1093... Val Loss: 1.2585\n",
      "Epoch: 23/30... Step: 3120... Loss: 1.1298... Val Loss: 1.2590\n",
      "Epoch: 23/30... Step: 3130... Loss: 1.1165... Val Loss: 1.2571\n",
      "Epoch: 23/30... Step: 3140... Loss: 1.0765... Val Loss: 1.2622\n",
      "Epoch: 23/30... Step: 3150... Loss: 1.1081... Val Loss: 1.2554\n",
      "Epoch: 23/30... Step: 3160... Loss: 1.1221... Val Loss: 1.2556\n",
      "Epoch: 23/30... Step: 3170... Loss: 1.1095... Val Loss: 1.2566\n",
      "Epoch: 23/30... Step: 3180... Loss: 1.0948... Val Loss: 1.2640\n",
      "Epoch: 23/30... Step: 3190... Loss: 1.1148... Val Loss: 1.2589\n",
      "Epoch: 24/30... Step: 3200... Loss: 1.1265... Val Loss: 1.2573\n",
      "Epoch: 24/30... Step: 3210... Loss: 1.0969... Val Loss: 1.2582\n",
      "Epoch: 24/30... Step: 3220... Loss: 1.1352... Val Loss: 1.2522\n",
      "Epoch: 24/30... Step: 3230... Loss: 1.0801... Val Loss: 1.2584\n",
      "Epoch: 24/30... Step: 3240... Loss: 1.0757... Val Loss: 1.2664\n",
      "Epoch: 24/30... Step: 3250... Loss: 1.1252... Val Loss: 1.2587\n",
      "Epoch: 24/30... Step: 3260... Loss: 1.1297... Val Loss: 1.2626\n",
      "Epoch: 24/30... Step: 3270... Loss: 1.1233... Val Loss: 1.2582\n",
      "Epoch: 24/30... Step: 3280... Loss: 1.1370... Val Loss: 1.2645\n",
      "Epoch: 24/30... Step: 3290... Loss: 1.1097... Val Loss: 1.2575\n",
      "Epoch: 24/30... Step: 3300... Loss: 1.1085... Val Loss: 1.2599\n",
      "Epoch: 24/30... Step: 3310... Loss: 1.1147... Val Loss: 1.2689\n",
      "Epoch: 24/30... Step: 3320... Loss: 1.0813... Val Loss: 1.2659\n",
      "Epoch: 24/30... Step: 3330... Loss: 1.1331... Val Loss: 1.2607\n",
      "Epoch: 25/30... Step: 3340... Loss: 1.1051... Val Loss: 1.2657\n",
      "Epoch: 25/30... Step: 3350... Loss: 1.1115... Val Loss: 1.2643\n",
      "Epoch: 25/30... Step: 3360... Loss: 1.1010... Val Loss: 1.2589\n",
      "Epoch: 25/30... Step: 3370... Loss: 1.1010... Val Loss: 1.2673\n",
      "Epoch: 25/30... Step: 3380... Loss: 1.1025... Val Loss: 1.2681\n",
      "Epoch: 25/30... Step: 3390... Loss: 1.0883... Val Loss: 1.2625\n",
      "Epoch: 25/30... Step: 3400... Loss: 1.1044... Val Loss: 1.2603\n",
      "Epoch: 25/30... Step: 3410... Loss: 1.1172... Val Loss: 1.2578\n",
      "Epoch: 25/30... Step: 3420... Loss: 1.0928... Val Loss: 1.2683\n",
      "Epoch: 25/30... Step: 3430... Loss: 1.1066... Val Loss: 1.2580\n",
      "Epoch: 25/30... Step: 3440... Loss: 1.1015... Val Loss: 1.2631\n",
      "Epoch: 25/30... Step: 3450... Loss: 1.0946... Val Loss: 1.2665\n",
      "Epoch: 25/30... Step: 3460... Loss: 1.1122... Val Loss: 1.2688\n",
      "Epoch: 25/30... Step: 3470... Loss: 1.1160... Val Loss: 1.2553\n",
      "Epoch: 26/30... Step: 3480... Loss: 1.1132... Val Loss: 1.2591\n",
      "Epoch: 26/30... Step: 3490... Loss: 1.1008... Val Loss: 1.2591\n",
      "Epoch: 26/30... Step: 3500... Loss: 1.0908... Val Loss: 1.2570\n",
      "Epoch: 26/30... Step: 3510... Loss: 1.1069... Val Loss: 1.2622\n",
      "Epoch: 26/30... Step: 3520... Loss: 1.0862... Val Loss: 1.2681\n",
      "Epoch: 26/30... Step: 3530... Loss: 1.0972... Val Loss: 1.2606\n",
      "Epoch: 26/30... Step: 3540... Loss: 1.1158... Val Loss: 1.2661\n",
      "Epoch: 26/30... Step: 3550... Loss: 1.0995... Val Loss: 1.2614\n",
      "Epoch: 26/30... Step: 3560... Loss: 1.0977... Val Loss: 1.2686\n",
      "Epoch: 26/30... Step: 3570... Loss: 1.0936... Val Loss: 1.2584\n",
      "Epoch: 26/30... Step: 3580... Loss: 1.1095... Val Loss: 1.2619\n",
      "Epoch: 26/30... Step: 3590... Loss: 1.0878... Val Loss: 1.2736\n",
      "Epoch: 26/30... Step: 3600... Loss: 1.0597... Val Loss: 1.2629\n",
      "Epoch: 26/30... Step: 3610... Loss: 1.1152... Val Loss: 1.2559\n",
      "Epoch: 27/30... Step: 3620... Loss: 1.0845... Val Loss: 1.2594\n",
      "Epoch: 27/30... Step: 3630... Loss: 1.0952... Val Loss: 1.2583\n",
      "Epoch: 27/30... Step: 3640... Loss: 1.0856... Val Loss: 1.2539\n",
      "Epoch: 27/30... Step: 3650... Loss: 1.0898... Val Loss: 1.2541\n",
      "Epoch: 27/30... Step: 3660... Loss: 1.0980... Val Loss: 1.2641\n",
      "Epoch: 27/30... Step: 3670... Loss: 1.1104... Val Loss: 1.2563\n",
      "Epoch: 27/30... Step: 3680... Loss: 1.1064... Val Loss: 1.2605\n",
      "Epoch: 27/30... Step: 3690... Loss: 1.0738... Val Loss: 1.2671\n",
      "Epoch: 27/30... Step: 3700... Loss: 1.0962... Val Loss: 1.2748\n",
      "Epoch: 27/30... Step: 3710... Loss: 1.0874... Val Loss: 1.2673\n",
      "Epoch: 27/30... Step: 3720... Loss: 1.0850... Val Loss: 1.2657\n",
      "Epoch: 27/30... Step: 3730... Loss: 1.1029... Val Loss: 1.2652\n",
      "Epoch: 27/30... Step: 3740... Loss: 1.0982... Val Loss: 1.2568\n",
      "Epoch: 27/30... Step: 3750... Loss: 1.1038... Val Loss: 1.2590\n",
      "Epoch: 28/30... Step: 3760... Loss: 1.0874... Val Loss: 1.2620\n",
      "Epoch: 28/30... Step: 3770... Loss: 1.0858... Val Loss: 1.2603\n",
      "Epoch: 28/30... Step: 3780... Loss: 1.0911... Val Loss: 1.2573\n",
      "Epoch: 28/30... Step: 3790... Loss: 1.1086... Val Loss: 1.2575\n",
      "Epoch: 28/30... Step: 3800... Loss: 1.0997... Val Loss: 1.2656\n",
      "Epoch: 28/30... Step: 3810... Loss: 1.0925... Val Loss: 1.2623\n",
      "Epoch: 28/30... Step: 3820... Loss: 1.1095... Val Loss: 1.2641\n",
      "Epoch: 28/30... Step: 3830... Loss: 1.0927... Val Loss: 1.2631\n",
      "Epoch: 28/30... Step: 3840... Loss: 1.0715... Val Loss: 1.2790\n",
      "Epoch: 28/30... Step: 3850... Loss: 1.0973... Val Loss: 1.2635\n",
      "Epoch: 28/30... Step: 3860... Loss: 1.0884... Val Loss: 1.2659\n",
      "Epoch: 28/30... Step: 3870... Loss: 1.0748... Val Loss: 1.2665\n",
      "Epoch: 28/30... Step: 3880... Loss: 1.0732... Val Loss: 1.2583\n",
      "Epoch: 28/30... Step: 3890... Loss: 1.0775... Val Loss: 1.2568\n",
      "Epoch: 29/30... Step: 3900... Loss: 1.0806... Val Loss: 1.2672\n",
      "Epoch: 29/30... Step: 3910... Loss: 1.1105... Val Loss: 1.2596\n",
      "Epoch: 29/30... Step: 3920... Loss: 1.1055... Val Loss: 1.2624\n",
      "Epoch: 29/30... Step: 3930... Loss: 1.1076... Val Loss: 1.2559\n",
      "Epoch: 29/30... Step: 3940... Loss: 1.0718... Val Loss: 1.2625\n",
      "Epoch: 29/30... Step: 3950... Loss: 1.0952... Val Loss: 1.2592\n",
      "Epoch: 29/30... Step: 3960... Loss: 1.0858... Val Loss: 1.2579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29/30... Step: 3970... Loss: 1.1122... Val Loss: 1.2616\n",
      "Epoch: 29/30... Step: 3980... Loss: 1.0858... Val Loss: 1.2749\n",
      "Epoch: 29/30... Step: 3990... Loss: 1.0739... Val Loss: 1.2690\n",
      "Epoch: 29/30... Step: 4000... Loss: 1.0867... Val Loss: 1.2671\n",
      "Epoch: 29/30... Step: 4010... Loss: 1.0781... Val Loss: 1.2689\n",
      "Epoch: 29/30... Step: 4020... Loss: 1.0737... Val Loss: 1.2606\n",
      "Epoch: 29/30... Step: 4030... Loss: 1.0967... Val Loss: 1.2620\n",
      "Epoch: 30/30... Step: 4040... Loss: 1.0795... Val Loss: 1.2555\n",
      "Epoch: 30/30... Step: 4050... Loss: 1.0946... Val Loss: 1.2539\n",
      "Epoch: 30/30... Step: 4060... Loss: 1.0953... Val Loss: 1.2616\n",
      "Epoch: 30/30... Step: 4070... Loss: 1.0883... Val Loss: 1.2548\n",
      "Epoch: 30/30... Step: 4080... Loss: 1.0861... Val Loss: 1.2619\n",
      "Epoch: 30/30... Step: 4090... Loss: 1.0974... Val Loss: 1.2623\n",
      "Epoch: 30/30... Step: 4100... Loss: 1.0715... Val Loss: 1.2649\n",
      "Epoch: 30/30... Step: 4110... Loss: 1.0705... Val Loss: 1.2623\n",
      "Epoch: 30/30... Step: 4120... Loss: 1.0607... Val Loss: 1.2793\n",
      "Epoch: 30/30... Step: 4130... Loss: 1.0623... Val Loss: 1.2704\n",
      "Epoch: 30/30... Step: 4140... Loss: 1.0745... Val Loss: 1.2670\n",
      "Epoch: 30/30... Step: 4150... Loss: 1.0705... Val Loss: 1.2667\n",
      "Epoch: 30/30... Step: 4160... Loss: 1.0940... Val Loss: 1.2634\n",
      "Epoch: 30/30... Step: 4170... Loss: 1.1390... Val Loss: 1.2609\n"
     ]
    }
   ],
   "source": [
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
