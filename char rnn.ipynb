{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Necessary Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:/Users/Geekquad/rnn_data/anna.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking out the first 500 characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverything was in confusion in the Oblonskys' house. The wife had\\ndiscovered that the husband was carrying on an intrigue with a French\\ngirl, who had been a governess in their family, and she had announced to\\nher husband that she could not go on living in the same house with him.\\nThis position of affairs had now lasted three days, and not only the\\nhusband and wife themselves, but all the members of their f\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cells below I am creating a couple of dictionaries to convert the characters to and from integers. \n",
    "Encoding the characters as integers makes it easier to use as input in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Creating two dictonaries\n",
    "   1. int2char : which maps integers to characters\n",
    "   2. char2int : which maps charaters to integers\"\"\"\n",
    "\n",
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate((chars)))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "#ENCODING THE TEXT:\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see those same characters from above, encoded as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([63, 17, 61, 71, 26,  4, 72, 28, 41,  5,  5,  5, 68, 61, 71, 71, 18,\n",
       "       28, 29, 61, 64, 35, 53, 35,  4, 73, 28, 61, 72,  4, 28, 61, 53, 53,\n",
       "       28, 61, 53, 35, 16,  4, 34, 28,  4, 48,  4, 72, 18, 28, 66, 58, 17,\n",
       "       61, 71, 71, 18, 28, 29, 61, 64, 35, 53, 18, 28, 35, 73, 28, 66, 58,\n",
       "       17, 61, 71, 71, 18, 28, 35, 58, 28, 35, 26, 73, 28, 22, 69, 58,  5,\n",
       "       69, 61, 18, 60,  5,  5, 74, 48,  4, 72, 18, 26, 17, 35, 58])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in out char-RNN, our LSTM expects an input that is one-hot encoded meaning, that each character is converted into an integer (by our created dictionary), and then converted into a column vector where only it's corresponding integer index will have the value of 1 and the rest of the vector will be filled with 0's. \n",
    "Making a one_hot_encoding function to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    one_hot = np.zeros((arr.size, n_labels), dtype = np.float32)\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "test_seq = np.array([[3, 5, 1]])\n",
    "one_hot = one_hot_encode(test_seq, 8)\n",
    "\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making training mini-batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train on this data, we will create mini-batches for training of some desired number of sequence steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    batch_size_total = batch_size*seq_length\n",
    "    n_batches = len(arr)//batch_size_total\n",
    "    \n",
    "    arr = arr[:n_batches*batch_size_total]\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:,:-1], y[:,-1] = x[:,1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll make some data sets and we can check out what's going on as we batch data. Here I am going to use a batch size of 8 and 50 sequence steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 8, 50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x/n [[63 17 61 71 26  4 72 28 41  5]\n",
      " [73 22 58 28 26 17 61 26 28 61]\n",
      " [ 4 58 52 28 22 72 28 61 28 29]\n",
      " [73 28 26 17  4 28 70 17 35  4]\n",
      " [28 73 61 69 28 17  4 72 28 26]\n",
      " [70 66 73 73 35 22 58 28 61 58]\n",
      " [28 15 58 58 61 28 17 61 52 28]\n",
      " [75 82 53 22 58 73 16 18 60 28]]\n",
      "\n",
      "y\n",
      " [[17 61 71 26  4 72 28 41  5  5]\n",
      " [22 58 28 26 17 61 26 28 61 26]\n",
      " [58 52 28 22 72 28 61 28 29 22]\n",
      " [28 26 17  4 28 70 17 35  4 29]\n",
      " [73 61 69 28 17  4 72 28 26  4]\n",
      " [66 73 73 35 22 58 28 61 58 52]\n",
      " [15 58 58 61 28 17 61 52 28 73]\n",
      " [82 53 22 58 73 16 18 60 28 12]]\n"
     ]
    }
   ],
   "source": [
    "print('x/n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2, drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        out = self.dropout(r_output)\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(), weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "        seq_length: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    net.train()\n",
    "    opt = torch.optim.Adam(net.parameters(), lr = lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            \n",
    "            counter += 1\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            net.zero_grad()\n",
    "            \n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y                                         \n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                net.train()\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "n_hidden=512\n",
    "n_layers=2\n",
    "\n",
    "net = CharRNN(chars, n_hidden, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "seq_length = 100\n",
    "n_epochs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30... Step: 10... Loss: 3.2533... Val Loss: 3.2208\n",
      "Epoch: 1/30... Step: 20... Loss: 3.1390... Val Loss: 3.1379\n",
      "Epoch: 1/30... Step: 30... Loss: 3.1390... Val Loss: 3.1244\n",
      "Epoch: 1/30... Step: 40... Loss: 3.1120... Val Loss: 3.1190\n",
      "Epoch: 1/30... Step: 50... Loss: 3.1443... Val Loss: 3.1165\n",
      "Epoch: 1/30... Step: 60... Loss: 3.1176... Val Loss: 3.1145\n",
      "Epoch: 1/30... Step: 70... Loss: 3.1058... Val Loss: 3.1115\n",
      "Epoch: 1/30... Step: 80... Loss: 3.1174... Val Loss: 3.1039\n",
      "Epoch: 1/30... Step: 90... Loss: 3.1050... Val Loss: 3.0855\n",
      "Epoch: 1/30... Step: 100... Loss: 3.0602... Val Loss: 3.0453\n",
      "Epoch: 1/30... Step: 110... Loss: 3.0394... Val Loss: 3.0287\n",
      "Epoch: 1/30... Step: 120... Loss: 2.9150... Val Loss: 2.8941\n",
      "Epoch: 1/30... Step: 130... Loss: 2.8789... Val Loss: 2.8259\n",
      "Epoch: 2/30... Step: 140... Loss: 2.7523... Val Loss: 2.6890\n",
      "Epoch: 2/30... Step: 150... Loss: 2.6459... Val Loss: 2.6020\n",
      "Epoch: 2/30... Step: 160... Loss: 2.5851... Val Loss: 2.5447\n",
      "Epoch: 2/30... Step: 170... Loss: 2.5162... Val Loss: 2.5033\n",
      "Epoch: 2/30... Step: 180... Loss: 2.4859... Val Loss: 2.4626\n",
      "Epoch: 2/30... Step: 190... Loss: 2.4350... Val Loss: 2.4300\n",
      "Epoch: 2/30... Step: 200... Loss: 2.4279... Val Loss: 2.3989\n",
      "Epoch: 2/30... Step: 210... Loss: 2.3898... Val Loss: 2.3674\n",
      "Epoch: 2/30... Step: 220... Loss: 2.3593... Val Loss: 2.3394\n",
      "Epoch: 2/30... Step: 230... Loss: 2.3541... Val Loss: 2.3370\n",
      "Epoch: 2/30... Step: 240... Loss: 2.3341... Val Loss: 2.2975\n",
      "Epoch: 2/30... Step: 250... Loss: 2.2612... Val Loss: 2.2649\n",
      "Epoch: 2/30... Step: 260... Loss: 2.2384... Val Loss: 2.2397\n",
      "Epoch: 2/30... Step: 270... Loss: 2.2428... Val Loss: 2.2108\n",
      "Epoch: 3/30... Step: 280... Loss: 2.2286... Val Loss: 2.1914\n",
      "Epoch: 3/30... Step: 290... Loss: 2.1870... Val Loss: 2.1646\n",
      "Epoch: 3/30... Step: 300... Loss: 2.1628... Val Loss: 2.1386\n",
      "Epoch: 3/30... Step: 310... Loss: 2.1425... Val Loss: 2.1242\n",
      "Epoch: 3/30... Step: 320... Loss: 2.1147... Val Loss: 2.0963\n",
      "Epoch: 3/30... Step: 330... Loss: 2.0863... Val Loss: 2.0847\n",
      "Epoch: 3/30... Step: 340... Loss: 2.0983... Val Loss: 2.0592\n",
      "Epoch: 3/30... Step: 350... Loss: 2.0792... Val Loss: 2.0409\n",
      "Epoch: 3/30... Step: 360... Loss: 2.0143... Val Loss: 2.0265\n",
      "Epoch: 3/30... Step: 370... Loss: 2.0417... Val Loss: 2.0131\n",
      "Epoch: 3/30... Step: 380... Loss: 2.0203... Val Loss: 1.9966\n",
      "Epoch: 3/30... Step: 390... Loss: 1.9905... Val Loss: 1.9780\n",
      "Epoch: 3/30... Step: 400... Loss: 1.9589... Val Loss: 1.9579\n",
      "Epoch: 3/30... Step: 410... Loss: 1.9657... Val Loss: 1.9454\n",
      "Epoch: 4/30... Step: 420... Loss: 1.9586... Val Loss: 1.9257\n",
      "Epoch: 4/30... Step: 430... Loss: 1.9464... Val Loss: 1.9113\n",
      "Epoch: 4/30... Step: 440... Loss: 1.9312... Val Loss: 1.9044\n",
      "Epoch: 4/30... Step: 450... Loss: 1.8653... Val Loss: 1.8818\n",
      "Epoch: 4/30... Step: 460... Loss: 1.8634... Val Loss: 1.8708\n",
      "Epoch: 4/30... Step: 470... Loss: 1.8877... Val Loss: 1.8591\n",
      "Epoch: 4/30... Step: 480... Loss: 1.8614... Val Loss: 1.8466\n",
      "Epoch: 4/30... Step: 490... Loss: 1.8704... Val Loss: 1.8384\n",
      "Epoch: 4/30... Step: 500... Loss: 1.8724... Val Loss: 1.8273\n",
      "Epoch: 4/30... Step: 510... Loss: 1.8425... Val Loss: 1.8158\n",
      "Epoch: 4/30... Step: 520... Loss: 1.8503... Val Loss: 1.8045\n",
      "Epoch: 4/30... Step: 530... Loss: 1.8104... Val Loss: 1.7936\n",
      "Epoch: 4/30... Step: 540... Loss: 1.7684... Val Loss: 1.7837\n",
      "Epoch: 4/30... Step: 550... Loss: 1.8125... Val Loss: 1.7690\n",
      "Epoch: 5/30... Step: 560... Loss: 1.7826... Val Loss: 1.7588\n",
      "Epoch: 5/30... Step: 570... Loss: 1.7640... Val Loss: 1.7488\n",
      "Epoch: 5/30... Step: 580... Loss: 1.7419... Val Loss: 1.7442\n",
      "Epoch: 5/30... Step: 590... Loss: 1.7500... Val Loss: 1.7298\n",
      "Epoch: 5/30... Step: 600... Loss: 1.7371... Val Loss: 1.7249\n",
      "Epoch: 5/30... Step: 610... Loss: 1.7207... Val Loss: 1.7146\n",
      "Epoch: 5/30... Step: 620... Loss: 1.7185... Val Loss: 1.7105\n",
      "Epoch: 5/30... Step: 630... Loss: 1.7341... Val Loss: 1.7059\n",
      "Epoch: 5/30... Step: 640... Loss: 1.7112... Val Loss: 1.6930\n",
      "Epoch: 5/30... Step: 650... Loss: 1.6945... Val Loss: 1.6843\n",
      "Epoch: 5/30... Step: 660... Loss: 1.6638... Val Loss: 1.6756\n",
      "Epoch: 5/30... Step: 670... Loss: 1.6952... Val Loss: 1.6721\n",
      "Epoch: 5/30... Step: 680... Loss: 1.6884... Val Loss: 1.6640\n",
      "Epoch: 5/30... Step: 690... Loss: 1.6688... Val Loss: 1.6591\n",
      "Epoch: 6/30... Step: 700... Loss: 1.6660... Val Loss: 1.6491\n",
      "Epoch: 6/30... Step: 710... Loss: 1.6586... Val Loss: 1.6461\n",
      "Epoch: 6/30... Step: 720... Loss: 1.6394... Val Loss: 1.6386\n",
      "Epoch: 6/30... Step: 730... Loss: 1.6525... Val Loss: 1.6298\n",
      "Epoch: 6/30... Step: 740... Loss: 1.6269... Val Loss: 1.6268\n",
      "Epoch: 6/30... Step: 750... Loss: 1.6098... Val Loss: 1.6200\n",
      "Epoch: 6/30... Step: 760... Loss: 1.6562... Val Loss: 1.6181\n",
      "Epoch: 6/30... Step: 770... Loss: 1.6194... Val Loss: 1.6134\n",
      "Epoch: 6/30... Step: 780... Loss: 1.6117... Val Loss: 1.6048\n",
      "Epoch: 6/30... Step: 790... Loss: 1.5978... Val Loss: 1.5980\n",
      "Epoch: 6/30... Step: 800... Loss: 1.6116... Val Loss: 1.5956\n",
      "Epoch: 6/30... Step: 810... Loss: 1.5936... Val Loss: 1.5942\n",
      "Epoch: 6/30... Step: 820... Loss: 1.5685... Val Loss: 1.5850\n",
      "Epoch: 6/30... Step: 830... Loss: 1.6087... Val Loss: 1.5835\n",
      "Epoch: 7/30... Step: 840... Loss: 1.5527... Val Loss: 1.5776\n",
      "Epoch: 7/30... Step: 850... Loss: 1.5748... Val Loss: 1.5701\n",
      "Epoch: 7/30... Step: 860... Loss: 1.5582... Val Loss: 1.5635\n",
      "Epoch: 7/30... Step: 870... Loss: 1.5685... Val Loss: 1.5590\n",
      "Epoch: 7/30... Step: 880... Loss: 1.5729... Val Loss: 1.5595\n",
      "Epoch: 7/30... Step: 890... Loss: 1.5734... Val Loss: 1.5520\n",
      "Epoch: 7/30... Step: 900... Loss: 1.5564... Val Loss: 1.5561\n",
      "Epoch: 7/30... Step: 910... Loss: 1.5165... Val Loss: 1.5488\n",
      "Epoch: 7/30... Step: 920... Loss: 1.5492... Val Loss: 1.5409\n",
      "Epoch: 7/30... Step: 930... Loss: 1.5292... Val Loss: 1.5381\n",
      "Epoch: 7/30... Step: 940... Loss: 1.5313... Val Loss: 1.5362\n",
      "Epoch: 7/30... Step: 950... Loss: 1.5395... Val Loss: 1.5313\n",
      "Epoch: 7/30... Step: 960... Loss: 1.5422... Val Loss: 1.5246\n",
      "Epoch: 7/30... Step: 970... Loss: 1.5450... Val Loss: 1.5271\n",
      "Epoch: 8/30... Step: 980... Loss: 1.5267... Val Loss: 1.5238\n",
      "Epoch: 8/30... Step: 990... Loss: 1.5231... Val Loss: 1.5164\n",
      "Epoch: 8/30... Step: 1000... Loss: 1.5153... Val Loss: 1.5104\n",
      "Epoch: 8/30... Step: 1010... Loss: 1.5550... Val Loss: 1.5064\n",
      "Epoch: 8/30... Step: 1020... Loss: 1.5195... Val Loss: 1.5099\n",
      "Epoch: 8/30... Step: 1030... Loss: 1.5076... Val Loss: 1.5068\n",
      "Epoch: 8/30... Step: 1040... Loss: 1.5077... Val Loss: 1.5035\n",
      "Epoch: 8/30... Step: 1050... Loss: 1.4945... Val Loss: 1.4982\n",
      "Epoch: 8/30... Step: 1060... Loss: 1.4840... Val Loss: 1.4956\n",
      "Epoch: 8/30... Step: 1070... Loss: 1.4971... Val Loss: 1.4897\n",
      "Epoch: 8/30... Step: 1080... Loss: 1.4888... Val Loss: 1.4874\n",
      "Epoch: 8/30... Step: 1090... Loss: 1.4818... Val Loss: 1.4817\n",
      "Epoch: 8/30... Step: 1100... Loss: 1.4689... Val Loss: 1.4806\n",
      "Epoch: 8/30... Step: 1110... Loss: 1.4761... Val Loss: 1.4786\n",
      "Epoch: 9/30... Step: 1120... Loss: 1.5005... Val Loss: 1.4801\n",
      "Epoch: 9/30... Step: 1130... Loss: 1.4858... Val Loss: 1.4774\n",
      "Epoch: 9/30... Step: 1140... Loss: 1.4792... Val Loss: 1.4706\n",
      "Epoch: 9/30... Step: 1150... Loss: 1.4918... Val Loss: 1.4703\n",
      "Epoch: 9/30... Step: 1160... Loss: 1.4479... Val Loss: 1.4694\n",
      "Epoch: 9/30... Step: 1170... Loss: 1.4587... Val Loss: 1.4655\n",
      "Epoch: 9/30... Step: 1180... Loss: 1.4496... Val Loss: 1.4634\n",
      "Epoch: 9/30... Step: 1190... Loss: 1.4830... Val Loss: 1.4607\n",
      "Epoch: 9/30... Step: 1200... Loss: 1.4387... Val Loss: 1.4572\n",
      "Epoch: 9/30... Step: 1210... Loss: 1.4486... Val Loss: 1.4548\n",
      "Epoch: 9/30... Step: 1220... Loss: 1.4444... Val Loss: 1.4561\n",
      "Epoch: 9/30... Step: 1230... Loss: 1.4337... Val Loss: 1.4484\n",
      "Epoch: 9/30... Step: 1240... Loss: 1.4324... Val Loss: 1.4459\n",
      "Epoch: 9/30... Step: 1250... Loss: 1.4438... Val Loss: 1.4472\n",
      "Epoch: 10/30... Step: 1260... Loss: 1.4364... Val Loss: 1.4444\n",
      "Epoch: 10/30... Step: 1270... Loss: 1.4315... Val Loss: 1.4411\n",
      "Epoch: 10/30... Step: 1280... Loss: 1.4474... Val Loss: 1.4393\n",
      "Epoch: 10/30... Step: 1290... Loss: 1.4452... Val Loss: 1.4430\n",
      "Epoch: 10/30... Step: 1300... Loss: 1.4311... Val Loss: 1.4377\n",
      "Epoch: 10/30... Step: 1310... Loss: 1.4365... Val Loss: 1.4340\n",
      "Epoch: 10/30... Step: 1320... Loss: 1.4054... Val Loss: 1.4309\n",
      "Epoch: 10/30... Step: 1330... Loss: 1.3999... Val Loss: 1.4346\n",
      "Epoch: 10/30... Step: 1340... Loss: 1.4061... Val Loss: 1.4274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/30... Step: 1350... Loss: 1.3886... Val Loss: 1.4249\n",
      "Epoch: 10/30... Step: 1360... Loss: 1.3932... Val Loss: 1.4251\n",
      "Epoch: 10/30... Step: 1370... Loss: 1.3878... Val Loss: 1.4262\n",
      "Epoch: 10/30... Step: 1380... Loss: 1.4275... Val Loss: 1.4179\n",
      "Epoch: 10/30... Step: 1390... Loss: 1.4311... Val Loss: 1.4175\n",
      "Epoch: 11/30... Step: 1400... Loss: 1.4430... Val Loss: 1.4201\n",
      "Epoch: 11/30... Step: 1410... Loss: 1.4420... Val Loss: 1.4179\n",
      "Epoch: 11/30... Step: 1420... Loss: 1.4298... Val Loss: 1.4111\n",
      "Epoch: 11/30... Step: 1430... Loss: 1.4014... Val Loss: 1.4199\n",
      "Epoch: 11/30... Step: 1440... Loss: 1.4299... Val Loss: 1.4167\n",
      "Epoch: 11/30... Step: 1450... Loss: 1.3541... Val Loss: 1.4075\n",
      "Epoch: 11/30... Step: 1460... Loss: 1.3813... Val Loss: 1.4072\n",
      "Epoch: 11/30... Step: 1470... Loss: 1.3738... Val Loss: 1.4110\n",
      "Epoch: 11/30... Step: 1480... Loss: 1.3919... Val Loss: 1.4040\n",
      "Epoch: 11/30... Step: 1490... Loss: 1.3715... Val Loss: 1.4010\n",
      "Epoch: 11/30... Step: 1500... Loss: 1.3719... Val Loss: 1.4049\n",
      "Epoch: 11/30... Step: 1510... Loss: 1.3553... Val Loss: 1.4012\n",
      "Epoch: 11/30... Step: 1520... Loss: 1.4000... Val Loss: 1.3974\n",
      "Epoch: 12/30... Step: 1530... Loss: 1.4349... Val Loss: 1.3973\n",
      "Epoch: 12/30... Step: 1540... Loss: 1.3973... Val Loss: 1.3971\n",
      "Epoch: 12/30... Step: 1550... Loss: 1.3990... Val Loss: 1.3940\n",
      "Epoch: 12/30... Step: 1560... Loss: 1.4094... Val Loss: 1.3904\n",
      "Epoch: 12/30... Step: 1570... Loss: 1.3560... Val Loss: 1.3941\n",
      "Epoch: 12/30... Step: 1580... Loss: 1.3434... Val Loss: 1.3944\n",
      "Epoch: 12/30... Step: 1590... Loss: 1.3306... Val Loss: 1.3898\n",
      "Epoch: 12/30... Step: 1600... Loss: 1.3617... Val Loss: 1.3877\n",
      "Epoch: 12/30... Step: 1610... Loss: 1.3556... Val Loss: 1.3939\n",
      "Epoch: 12/30... Step: 1620... Loss: 1.3502... Val Loss: 1.3837\n",
      "Epoch: 12/30... Step: 1630... Loss: 1.3681... Val Loss: 1.3854\n",
      "Epoch: 12/30... Step: 1640... Loss: 1.3538... Val Loss: 1.3852\n",
      "Epoch: 12/30... Step: 1650... Loss: 1.3245... Val Loss: 1.3813\n",
      "Epoch: 12/30... Step: 1660... Loss: 1.3811... Val Loss: 1.3775\n",
      "Epoch: 13/30... Step: 1670... Loss: 1.3489... Val Loss: 1.3829\n",
      "Epoch: 13/30... Step: 1680... Loss: 1.3619... Val Loss: 1.3770\n",
      "Epoch: 13/30... Step: 1690... Loss: 1.3462... Val Loss: 1.3761\n",
      "Epoch: 13/30... Step: 1700... Loss: 1.3458... Val Loss: 1.3746\n",
      "Epoch: 13/30... Step: 1710... Loss: 1.3171... Val Loss: 1.3781\n",
      "Epoch: 13/30... Step: 1720... Loss: 1.3339... Val Loss: 1.3718\n",
      "Epoch: 13/30... Step: 1730... Loss: 1.3671... Val Loss: 1.3675\n",
      "Epoch: 13/30... Step: 1740... Loss: 1.3297... Val Loss: 1.3681\n",
      "Epoch: 13/30... Step: 1750... Loss: 1.3019... Val Loss: 1.3736\n",
      "Epoch: 13/30... Step: 1760... Loss: 1.3305... Val Loss: 1.3695\n",
      "Epoch: 13/30... Step: 1770... Loss: 1.3475... Val Loss: 1.3643\n",
      "Epoch: 13/30... Step: 1780... Loss: 1.3211... Val Loss: 1.3647\n",
      "Epoch: 13/30... Step: 1790... Loss: 1.3197... Val Loss: 1.3620\n",
      "Epoch: 13/30... Step: 1800... Loss: 1.3353... Val Loss: 1.3644\n",
      "Epoch: 14/30... Step: 1810... Loss: 1.3350... Val Loss: 1.3612\n",
      "Epoch: 14/30... Step: 1820... Loss: 1.3281... Val Loss: 1.3610\n",
      "Epoch: 14/30... Step: 1830... Loss: 1.3366... Val Loss: 1.3606\n",
      "Epoch: 14/30... Step: 1840... Loss: 1.2858... Val Loss: 1.3564\n",
      "Epoch: 14/30... Step: 1850... Loss: 1.2773... Val Loss: 1.3586\n",
      "Epoch: 14/30... Step: 1860... Loss: 1.3335... Val Loss: 1.3585\n",
      "Epoch: 14/30... Step: 1870... Loss: 1.3376... Val Loss: 1.3519\n",
      "Epoch: 14/30... Step: 1880... Loss: 1.3288... Val Loss: 1.3530\n",
      "Epoch: 14/30... Step: 1890... Loss: 1.3418... Val Loss: 1.3572\n",
      "Epoch: 14/30... Step: 1900... Loss: 1.3229... Val Loss: 1.3507\n",
      "Epoch: 14/30... Step: 1910... Loss: 1.3290... Val Loss: 1.3507\n",
      "Epoch: 14/30... Step: 1920... Loss: 1.3214... Val Loss: 1.3493\n",
      "Epoch: 14/30... Step: 1930... Loss: 1.2783... Val Loss: 1.3454\n",
      "Epoch: 14/30... Step: 1940... Loss: 1.3473... Val Loss: 1.3462\n",
      "Epoch: 15/30... Step: 1950... Loss: 1.3063... Val Loss: 1.3482\n",
      "Epoch: 15/30... Step: 1960... Loss: 1.3169... Val Loss: 1.3457\n",
      "Epoch: 15/30... Step: 1970... Loss: 1.3108... Val Loss: 1.3461\n",
      "Epoch: 15/30... Step: 1980... Loss: 1.2950... Val Loss: 1.3467\n",
      "Epoch: 15/30... Step: 1990... Loss: 1.3013... Val Loss: 1.3468\n",
      "Epoch: 15/30... Step: 2000... Loss: 1.2812... Val Loss: 1.3492\n",
      "Epoch: 15/30... Step: 2010... Loss: 1.2951... Val Loss: 1.3390\n",
      "Epoch: 15/30... Step: 2020... Loss: 1.3168... Val Loss: 1.3410\n",
      "Epoch: 15/30... Step: 2030... Loss: 1.2960... Val Loss: 1.3422\n",
      "Epoch: 15/30... Step: 2040... Loss: 1.3098... Val Loss: 1.3396\n",
      "Epoch: 15/30... Step: 2050... Loss: 1.2861... Val Loss: 1.3404\n",
      "Epoch: 15/30... Step: 2060... Loss: 1.3021... Val Loss: 1.3374\n",
      "Epoch: 15/30... Step: 2070... Loss: 1.3014... Val Loss: 1.3372\n",
      "Epoch: 15/30... Step: 2080... Loss: 1.2979... Val Loss: 1.3368\n",
      "Epoch: 16/30... Step: 2090... Loss: 1.3006... Val Loss: 1.3378\n",
      "Epoch: 16/30... Step: 2100... Loss: 1.2943... Val Loss: 1.3334\n",
      "Epoch: 16/30... Step: 2110... Loss: 1.2841... Val Loss: 1.3380\n",
      "Epoch: 16/30... Step: 2120... Loss: 1.2991... Val Loss: 1.3367\n",
      "Epoch: 16/30... Step: 2130... Loss: 1.2777... Val Loss: 1.3359\n",
      "Epoch: 16/30... Step: 2140... Loss: 1.2805... Val Loss: 1.3345\n",
      "Epoch: 16/30... Step: 2150... Loss: 1.3131... Val Loss: 1.3303\n",
      "Epoch: 16/30... Step: 2160... Loss: 1.2864... Val Loss: 1.3342\n",
      "Epoch: 16/30... Step: 2170... Loss: 1.2830... Val Loss: 1.3338\n",
      "Epoch: 16/30... Step: 2180... Loss: 1.2706... Val Loss: 1.3301\n",
      "Epoch: 16/30... Step: 2190... Loss: 1.2902... Val Loss: 1.3284\n",
      "Epoch: 16/30... Step: 2200... Loss: 1.2717... Val Loss: 1.3297\n",
      "Epoch: 16/30... Step: 2210... Loss: 1.2438... Val Loss: 1.3285\n",
      "Epoch: 16/30... Step: 2220... Loss: 1.2922... Val Loss: 1.3281\n",
      "Epoch: 17/30... Step: 2230... Loss: 1.2599... Val Loss: 1.3256\n",
      "Epoch: 17/30... Step: 2240... Loss: 1.2830... Val Loss: 1.3214\n",
      "Epoch: 17/30... Step: 2250... Loss: 1.2590... Val Loss: 1.3255\n",
      "Epoch: 17/30... Step: 2260... Loss: 1.2647... Val Loss: 1.3213\n",
      "Epoch: 17/30... Step: 2270... Loss: 1.2799... Val Loss: 1.3214\n",
      "Epoch: 17/30... Step: 2280... Loss: 1.2936... Val Loss: 1.3176\n",
      "Epoch: 17/30... Step: 2290... Loss: 1.2853... Val Loss: 1.3198\n",
      "Epoch: 17/30... Step: 2300... Loss: 1.2369... Val Loss: 1.3253\n",
      "Epoch: 17/30... Step: 2310... Loss: 1.2667... Val Loss: 1.3229\n",
      "Epoch: 17/30... Step: 2320... Loss: 1.2651... Val Loss: 1.3191\n",
      "Epoch: 17/30... Step: 2330... Loss: 1.2633... Val Loss: 1.3176\n",
      "Epoch: 17/30... Step: 2340... Loss: 1.2770... Val Loss: 1.3159\n",
      "Epoch: 17/30... Step: 2350... Loss: 1.2815... Val Loss: 1.3174\n",
      "Epoch: 17/30... Step: 2360... Loss: 1.2885... Val Loss: 1.3195\n",
      "Epoch: 18/30... Step: 2370... Loss: 1.2566... Val Loss: 1.3184\n",
      "Epoch: 18/30... Step: 2380... Loss: 1.2591... Val Loss: 1.3165\n",
      "Epoch: 18/30... Step: 2390... Loss: 1.2650... Val Loss: 1.3183\n",
      "Epoch: 18/30... Step: 2400... Loss: 1.2851... Val Loss: 1.3161\n",
      "Epoch: 18/30... Step: 2410... Loss: 1.2759... Val Loss: 1.3152\n",
      "Epoch: 18/30... Step: 2420... Loss: 1.2618... Val Loss: 1.3123\n",
      "Epoch: 18/30... Step: 2430... Loss: 1.2728... Val Loss: 1.3098\n",
      "Epoch: 18/30... Step: 2440... Loss: 1.2481... Val Loss: 1.3137\n",
      "Epoch: 18/30... Step: 2450... Loss: 1.2515... Val Loss: 1.3130\n",
      "Epoch: 18/30... Step: 2460... Loss: 1.2589... Val Loss: 1.3123\n",
      "Epoch: 18/30... Step: 2470... Loss: 1.2524... Val Loss: 1.3115\n",
      "Epoch: 18/30... Step: 2480... Loss: 1.2450... Val Loss: 1.3101\n",
      "Epoch: 18/30... Step: 2490... Loss: 1.2323... Val Loss: 1.3082\n",
      "Epoch: 18/30... Step: 2500... Loss: 1.2418... Val Loss: 1.3077\n",
      "Epoch: 19/30... Step: 2510... Loss: 1.2523... Val Loss: 1.3109\n",
      "Epoch: 19/30... Step: 2520... Loss: 1.2569... Val Loss: 1.3095\n",
      "Epoch: 19/30... Step: 2530... Loss: 1.2683... Val Loss: 1.3097\n",
      "Epoch: 19/30... Step: 2540... Loss: 1.2712... Val Loss: 1.3064\n",
      "Epoch: 19/30... Step: 2550... Loss: 1.2357... Val Loss: 1.3087\n",
      "Epoch: 19/30... Step: 2560... Loss: 1.2544... Val Loss: 1.3024\n",
      "Epoch: 19/30... Step: 2570... Loss: 1.2415... Val Loss: 1.3009\n",
      "Epoch: 19/30... Step: 2580... Loss: 1.2807... Val Loss: 1.3061\n",
      "Epoch: 19/30... Step: 2590... Loss: 1.2337... Val Loss: 1.3046\n",
      "Epoch: 19/30... Step: 2600... Loss: 1.2301... Val Loss: 1.3042\n",
      "Epoch: 19/30... Step: 2610... Loss: 1.2438... Val Loss: 1.3036\n",
      "Epoch: 19/30... Step: 2620... Loss: 1.2240... Val Loss: 1.3015\n",
      "Epoch: 19/30... Step: 2630... Loss: 1.2423... Val Loss: 1.3084\n",
      "Epoch: 19/30... Step: 2640... Loss: 1.2577... Val Loss: 1.3087\n",
      "Epoch: 20/30... Step: 2650... Loss: 1.2566... Val Loss: 1.3003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/30... Step: 2660... Loss: 1.2462... Val Loss: 1.3030\n",
      "Epoch: 20/30... Step: 2670... Loss: 1.2675... Val Loss: 1.3043\n",
      "Epoch: 20/30... Step: 2680... Loss: 1.2495... Val Loss: 1.2980\n",
      "Epoch: 20/30... Step: 2690... Loss: 1.2341... Val Loss: 1.3015\n",
      "Epoch: 20/30... Step: 2700... Loss: 1.2544... Val Loss: 1.2965\n",
      "Epoch: 20/30... Step: 2710... Loss: 1.2274... Val Loss: 1.2954\n",
      "Epoch: 20/30... Step: 2720... Loss: 1.2270... Val Loss: 1.3000\n",
      "Epoch: 20/30... Step: 2730... Loss: 1.2094... Val Loss: 1.2996\n",
      "Epoch: 20/30... Step: 2740... Loss: 1.2105... Val Loss: 1.2962\n",
      "Epoch: 20/30... Step: 2750... Loss: 1.2141... Val Loss: 1.2957\n",
      "Epoch: 20/30... Step: 2760... Loss: 1.2122... Val Loss: 1.2957\n",
      "Epoch: 20/30... Step: 2770... Loss: 1.2523... Val Loss: 1.2955\n",
      "Epoch: 20/30... Step: 2780... Loss: 1.2833... Val Loss: 1.2940\n",
      "Epoch: 21/30... Step: 2790... Loss: 1.2518... Val Loss: 1.2947\n",
      "Epoch: 21/30... Step: 2800... Loss: 1.2664... Val Loss: 1.2993\n",
      "Epoch: 21/30... Step: 2810... Loss: 1.2614... Val Loss: 1.2967\n",
      "Epoch: 21/30... Step: 2820... Loss: 1.2354... Val Loss: 1.2923\n",
      "Epoch: 21/30... Step: 2830... Loss: 1.2544... Val Loss: 1.2950\n",
      "Epoch: 21/30... Step: 2840... Loss: 1.1941... Val Loss: 1.2916\n",
      "Epoch: 21/30... Step: 2850... Loss: 1.2129... Val Loss: 1.2926\n",
      "Epoch: 21/30... Step: 2860... Loss: 1.2020... Val Loss: 1.2955\n",
      "Epoch: 21/30... Step: 2870... Loss: 1.2329... Val Loss: 1.2927\n",
      "Epoch: 21/30... Step: 2880... Loss: 1.2186... Val Loss: 1.2913\n",
      "Epoch: 21/30... Step: 2890... Loss: 1.2085... Val Loss: 1.2933\n",
      "Epoch: 21/30... Step: 2900... Loss: 1.1948... Val Loss: 1.2906\n",
      "Epoch: 21/30... Step: 2910... Loss: 1.2311... Val Loss: 1.2896\n",
      "Epoch: 22/30... Step: 2920... Loss: 1.2991... Val Loss: 1.2891\n",
      "Epoch: 22/30... Step: 2930... Loss: 1.2356... Val Loss: 1.2906\n",
      "Epoch: 22/30... Step: 2940... Loss: 1.2340... Val Loss: 1.2932\n",
      "Epoch: 22/30... Step: 2950... Loss: 1.2462... Val Loss: 1.2900\n",
      "Epoch: 22/30... Step: 2960... Loss: 1.2120... Val Loss: 1.2866\n",
      "Epoch: 22/30... Step: 2970... Loss: 1.1983... Val Loss: 1.2898\n",
      "Epoch: 22/30... Step: 2980... Loss: 1.1890... Val Loss: 1.2863\n",
      "Epoch: 22/30... Step: 2990... Loss: 1.2099... Val Loss: 1.2860\n",
      "Epoch: 22/30... Step: 3000... Loss: 1.2077... Val Loss: 1.2909\n",
      "Epoch: 22/30... Step: 3010... Loss: 1.2017... Val Loss: 1.2903\n",
      "Epoch: 22/30... Step: 3020... Loss: 1.2232... Val Loss: 1.2900\n",
      "Epoch: 22/30... Step: 3030... Loss: 1.2041... Val Loss: 1.2919\n",
      "Epoch: 22/30... Step: 3040... Loss: 1.1789... Val Loss: 1.2870\n",
      "Epoch: 22/30... Step: 3050... Loss: 1.2411... Val Loss: 1.2866\n",
      "Epoch: 23/30... Step: 3060... Loss: 1.2078... Val Loss: 1.2892\n",
      "Epoch: 23/30... Step: 3070... Loss: 1.2256... Val Loss: 1.2881\n",
      "Epoch: 23/30... Step: 3080... Loss: 1.2055... Val Loss: 1.2817\n",
      "Epoch: 23/30... Step: 3090... Loss: 1.1981... Val Loss: 1.2825\n",
      "Epoch: 23/30... Step: 3100... Loss: 1.1812... Val Loss: 1.2863\n",
      "Epoch: 23/30... Step: 3110... Loss: 1.1857... Val Loss: 1.2880\n",
      "Epoch: 23/30... Step: 3120... Loss: 1.2278... Val Loss: 1.2859\n",
      "Epoch: 23/30... Step: 3130... Loss: 1.2017... Val Loss: 1.2816\n",
      "Epoch: 23/30... Step: 3140... Loss: 1.1648... Val Loss: 1.2831\n",
      "Epoch: 23/30... Step: 3150... Loss: 1.1963... Val Loss: 1.2838\n",
      "Epoch: 23/30... Step: 3160... Loss: 1.2123... Val Loss: 1.2854\n",
      "Epoch: 23/30... Step: 3170... Loss: 1.1892... Val Loss: 1.2847\n",
      "Epoch: 23/30... Step: 3180... Loss: 1.1826... Val Loss: 1.2832\n",
      "Epoch: 23/30... Step: 3190... Loss: 1.2069... Val Loss: 1.2820\n",
      "Epoch: 24/30... Step: 3200... Loss: 1.2169... Val Loss: 1.2845\n",
      "Epoch: 24/30... Step: 3210... Loss: 1.1932... Val Loss: 1.2803\n",
      "Epoch: 24/30... Step: 3220... Loss: 1.2116... Val Loss: 1.2767\n",
      "Epoch: 24/30... Step: 3230... Loss: 1.1706... Val Loss: 1.2812\n",
      "Epoch: 24/30... Step: 3240... Loss: 1.1686... Val Loss: 1.2830\n",
      "Epoch: 24/30... Step: 3250... Loss: 1.2088... Val Loss: 1.2844\n",
      "Epoch: 24/30... Step: 3260... Loss: 1.2117... Val Loss: 1.2795\n",
      "Epoch: 24/30... Step: 3270... Loss: 1.2083... Val Loss: 1.2772\n",
      "Epoch: 24/30... Step: 3280... Loss: 1.2237... Val Loss: 1.2822\n",
      "Epoch: 24/30... Step: 3290... Loss: 1.1976... Val Loss: 1.2794\n",
      "Epoch: 24/30... Step: 3300... Loss: 1.2038... Val Loss: 1.2813\n",
      "Epoch: 24/30... Step: 3310... Loss: 1.2046... Val Loss: 1.2816\n",
      "Epoch: 24/30... Step: 3320... Loss: 1.1677... Val Loss: 1.2780\n",
      "Epoch: 24/30... Step: 3330... Loss: 1.2201... Val Loss: 1.2764\n",
      "Epoch: 25/30... Step: 3340... Loss: 1.2018... Val Loss: 1.2834\n",
      "Epoch: 25/30... Step: 3350... Loss: 1.1972... Val Loss: 1.2781\n",
      "Epoch: 25/30... Step: 3360... Loss: 1.1906... Val Loss: 1.2762\n",
      "Epoch: 25/30... Step: 3370... Loss: 1.1942... Val Loss: 1.2775\n",
      "Epoch: 25/30... Step: 3380... Loss: 1.1874... Val Loss: 1.2821\n",
      "Epoch: 25/30... Step: 3390... Loss: 1.1725... Val Loss: 1.2859\n",
      "Epoch: 25/30... Step: 3400... Loss: 1.1844... Val Loss: 1.2814\n",
      "Epoch: 25/30... Step: 3410... Loss: 1.2091... Val Loss: 1.2815\n",
      "Epoch: 25/30... Step: 3420... Loss: 1.1845... Val Loss: 1.2829\n",
      "Epoch: 25/30... Step: 3430... Loss: 1.1923... Val Loss: 1.2816\n",
      "Epoch: 25/30... Step: 3440... Loss: 1.1749... Val Loss: 1.2820\n",
      "Epoch: 25/30... Step: 3450... Loss: 1.1874... Val Loss: 1.2782\n",
      "Epoch: 25/30... Step: 3460... Loss: 1.1992... Val Loss: 1.2778\n",
      "Epoch: 25/30... Step: 3470... Loss: 1.1967... Val Loss: 1.2756\n",
      "Epoch: 26/30... Step: 3480... Loss: 1.1973... Val Loss: 1.2814\n",
      "Epoch: 26/30... Step: 3490... Loss: 1.1877... Val Loss: 1.2765\n",
      "Epoch: 26/30... Step: 3500... Loss: 1.1788... Val Loss: 1.2729\n",
      "Epoch: 26/30... Step: 3510... Loss: 1.1942... Val Loss: 1.2761\n",
      "Epoch: 26/30... Step: 3520... Loss: 1.1671... Val Loss: 1.2793\n",
      "Epoch: 26/30... Step: 3530... Loss: 1.1731... Val Loss: 1.2777\n",
      "Epoch: 26/30... Step: 3540... Loss: 1.2032... Val Loss: 1.2756\n",
      "Epoch: 26/30... Step: 3550... Loss: 1.1731... Val Loss: 1.2756\n",
      "Epoch: 26/30... Step: 3560... Loss: 1.1730... Val Loss: 1.2765\n",
      "Epoch: 26/30... Step: 3570... Loss: 1.1695... Val Loss: 1.2773\n",
      "Epoch: 26/30... Step: 3580... Loss: 1.1922... Val Loss: 1.2750\n",
      "Epoch: 26/30... Step: 3590... Loss: 1.1820... Val Loss: 1.2728\n",
      "Epoch: 26/30... Step: 3600... Loss: 1.1411... Val Loss: 1.2764\n",
      "Epoch: 26/30... Step: 3610... Loss: 1.1827... Val Loss: 1.2712\n",
      "Epoch: 27/30... Step: 3620... Loss: 1.1718... Val Loss: 1.2726\n",
      "Epoch: 27/30... Step: 3630... Loss: 1.1778... Val Loss: 1.2696\n",
      "Epoch: 27/30... Step: 3640... Loss: 1.1620... Val Loss: 1.2700\n",
      "Epoch: 27/30... Step: 3650... Loss: 1.1690... Val Loss: 1.2724\n",
      "Epoch: 27/30... Step: 3660... Loss: 1.1832... Val Loss: 1.2760\n",
      "Epoch: 27/30... Step: 3670... Loss: 1.1878... Val Loss: 1.2751\n",
      "Epoch: 27/30... Step: 3680... Loss: 1.1953... Val Loss: 1.2724\n",
      "Epoch: 27/30... Step: 3690... Loss: 1.1537... Val Loss: 1.2690\n",
      "Epoch: 27/30... Step: 3700... Loss: 1.1704... Val Loss: 1.2706\n",
      "Epoch: 27/30... Step: 3710... Loss: 1.1740... Val Loss: 1.2717\n",
      "Epoch: 27/30... Step: 3720... Loss: 1.1649... Val Loss: 1.2700\n",
      "Epoch: 27/30... Step: 3730... Loss: 1.1865... Val Loss: 1.2692\n",
      "Epoch: 27/30... Step: 3740... Loss: 1.1924... Val Loss: 1.2728\n",
      "Epoch: 27/30... Step: 3750... Loss: 1.1836... Val Loss: 1.2679\n",
      "Epoch: 28/30... Step: 3760... Loss: 1.1602... Val Loss: 1.2708\n",
      "Epoch: 28/30... Step: 3770... Loss: 1.1734... Val Loss: 1.2672\n",
      "Epoch: 28/30... Step: 3780... Loss: 1.1661... Val Loss: 1.2704\n",
      "Epoch: 28/30... Step: 3790... Loss: 1.1910... Val Loss: 1.2737\n",
      "Epoch: 28/30... Step: 3800... Loss: 1.1831... Val Loss: 1.2721\n",
      "Epoch: 28/30... Step: 3810... Loss: 1.1730... Val Loss: 1.2681\n",
      "Epoch: 28/30... Step: 3820... Loss: 1.1861... Val Loss: 1.2703\n",
      "Epoch: 28/30... Step: 3830... Loss: 1.1676... Val Loss: 1.2674\n",
      "Epoch: 28/30... Step: 3840... Loss: 1.1540... Val Loss: 1.2668\n",
      "Epoch: 28/30... Step: 3850... Loss: 1.1712... Val Loss: 1.2665\n",
      "Epoch: 28/30... Step: 3860... Loss: 1.1683... Val Loss: 1.2659\n",
      "Epoch: 28/30... Step: 3870... Loss: 1.1621... Val Loss: 1.2660\n",
      "Epoch: 28/30... Step: 3880... Loss: 1.1460... Val Loss: 1.2688\n",
      "Epoch: 28/30... Step: 3890... Loss: 1.1580... Val Loss: 1.2663\n",
      "Epoch: 29/30... Step: 3900... Loss: 1.1629... Val Loss: 1.2653\n",
      "Epoch: 29/30... Step: 3910... Loss: 1.1772... Val Loss: 1.2609\n",
      "Epoch: 29/30... Step: 3920... Loss: 1.1744... Val Loss: 1.2646\n",
      "Epoch: 29/30... Step: 3930... Loss: 1.1827... Val Loss: 1.2670\n",
      "Epoch: 29/30... Step: 3940... Loss: 1.1547... Val Loss: 1.2687\n",
      "Epoch: 29/30... Step: 3950... Loss: 1.1651... Val Loss: 1.2655\n",
      "Epoch: 29/30... Step: 3960... Loss: 1.1561... Val Loss: 1.2633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29/30... Step: 3970... Loss: 1.1826... Val Loss: 1.2634\n",
      "Epoch: 29/30... Step: 3980... Loss: 1.1583... Val Loss: 1.2615\n",
      "Epoch: 29/30... Step: 3990... Loss: 1.1576... Val Loss: 1.2652\n",
      "Epoch: 29/30... Step: 4000... Loss: 1.1693... Val Loss: 1.2661\n",
      "Epoch: 29/30... Step: 4010... Loss: 1.1414... Val Loss: 1.2613\n",
      "Epoch: 29/30... Step: 4020... Loss: 1.1483... Val Loss: 1.2648\n",
      "Epoch: 29/30... Step: 4030... Loss: 1.1615... Val Loss: 1.2637\n",
      "Epoch: 30/30... Step: 4040... Loss: 1.1633... Val Loss: 1.2637\n",
      "Epoch: 30/30... Step: 4050... Loss: 1.1742... Val Loss: 1.2594\n",
      "Epoch: 30/30... Step: 4060... Loss: 1.1796... Val Loss: 1.2631\n",
      "Epoch: 30/30... Step: 4070... Loss: 1.1707... Val Loss: 1.2651\n",
      "Epoch: 30/30... Step: 4080... Loss: 1.1556... Val Loss: 1.2637\n",
      "Epoch: 30/30... Step: 4090... Loss: 1.1744... Val Loss: 1.2644\n",
      "Epoch: 30/30... Step: 4100... Loss: 1.1414... Val Loss: 1.2631\n",
      "Epoch: 30/30... Step: 4110... Loss: 1.1450... Val Loss: 1.2613\n",
      "Epoch: 30/30... Step: 4120... Loss: 1.1410... Val Loss: 1.2625\n",
      "Epoch: 30/30... Step: 4130... Loss: 1.1385... Val Loss: 1.2620\n",
      "Epoch: 30/30... Step: 4140... Loss: 1.1484... Val Loss: 1.2661\n",
      "Epoch: 30/30... Step: 4150... Loss: 1.1320... Val Loss: 1.2627\n",
      "Epoch: 30/30... Step: 4160... Loss: 1.1713... Val Loss: 1.2617\n",
      "Epoch: 30/30... Step: 4170... Loss: 1.1968... Val Loss: 1.2567\n"
     ]
    }
   ],
   "source": [
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we will save the model so we can load it again later is we need to. I am saving the parameters needed to create the same architecture, the hidden layer hyperparameters and the next characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'rnn_30_epoch.net'\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens' : net.chars}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that out mddol is trained, we can make predictions about the next characters. We can sample the text and make it more resoanble to handle.\n",
    "To sample, we pass in a character and have the network predict the next character. Then we take that character, pass it back in, and get another predicted character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### To make the prediction more resonable we will do \"Top K - sampling.\" \n",
    "Thsi will prevent the network from giving us completely absurd characters while allowing it to introduce some noise and randomness into the sampled text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, char, h = None, top_k = None):\n",
    "    x = np.array([[net.char2int[char]]])\n",
    "    x = one_hot_encode(x, len(net.chars))\n",
    "    inputs = torch.from_numpy(x)\n",
    "    \n",
    "    h = tuple([each.data for each in h])\n",
    "    out, h = net(inputs, h)\n",
    "    \n",
    "    p = F.softmax(out, dim =1).data\n",
    "    \n",
    "    #GETTING TOP CHARACTERS\n",
    "    \n",
    "    if top_k is None:\n",
    "        top_ch = np.arange(len(net.chars))\n",
    "    else:\n",
    "        p, top_ch = p.topk(top_k)\n",
    "        top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "    p = p.numpy().squeeze()\n",
    "    char = np.random.choice(top_ch, p=p/p.sum())\n",
    "    \n",
    "    return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Priming and Generating Texts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Priming otherwise the network will start out generating characters at random. In general the first bunch of characters will be a little rough since it hasn't built up a long history of characters to predict from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None):\n",
    "    net.eval() \n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aditya on the railways so in women as something would be delighted,\n",
      "but he went on; \"both the secret convensed officer of her hers in\n",
      "the\n",
      "way he has so minuteful, but to me, you define him,\"\n",
      "she asked, who had said a matter about his wheak as she had always said\n",
      "all the sister-in-law to see him all the path,\n",
      "and he would not come. But her son were all the point on the conversation with his\n",
      "significance with him.\n",
      "\n",
      "\"What do you say about! It's being might be time.\"\n",
      "\n",
      "\"I don't understand what I done anything,\" he thought.\n",
      "\n",
      "The side, too had\n",
      "taken them and so anything standing the most friends of a sister. All of his strange, hondrack and their states of their\n",
      "spart at the point of a chill. He came out of the sound of this man, significantly\n",
      "at his book.\n",
      "\n",
      "\"Yes, that's the mistale. What am the princess's a simple son there?\"\n",
      "\n",
      "\"Oh, yes, I won't think that I shall have a love to be anything to think of you.\"\n",
      "\n",
      "Stave was beating a person.\n",
      "\n",
      "She did not know the forest and her\n",
      "substaist, he settled her fa\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 1000, prime='Aditya ', top_k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Loading the checkoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rnn_30_epoch.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    \n",
    "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample using the Loaded Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Geekquad to his way in it. The life in enting sounds, and the sudden obsires\n",
      "of\n",
      "his left flying close and simple sounds and bedoother's face in their despatro department in\n",
      "the bails only one of outsider and from his bidget, and has handed his eyes, and as he should not give this;\n",
      "then were sole feeling by now, to his\n",
      "position--there was nothing but a teals still had to\n",
      "sit down. Only filled her terrible\n",
      "besige, seeing Lizaveta Petrovna to\n",
      "his whole discussion so as for his what they were phusping in from the\n",
      "curtsing in the\n",
      "conversation with Alexey Alexandrovitch's head.\n",
      "\n",
      "\n",
      "\n",
      "Chapter 22\n",
      "\n",
      "\n",
      "When he had never come up to him, and that the predict of his caped the\n",
      "coachman could see worse, so at that\n",
      "duries of that, too, to small, but he had been both her hander.\n",
      "\n",
      "\"I beg now and conditions away, but if no one\n",
      "can't except me of interest, but I cannot believe.\"\n",
      "\n",
      "She listened, but she flung ooter out\n",
      "of side, starting away from his hat, but she\n",
      "would not look at her. The condessed official cut. He saw that\n",
      "one one had all\n",
      "finished that he had gone\n",
      "up to his music and the trass.\n",
      "\n",
      "\"I had fault, why I\n",
      "will go to her methods of the carriage, and she's so lending it all the weed.\n",
      "The public little poars as to scrien and\n",
      "thespefflengy, and it was many.\n",
      "I'm asked.... Who is she to blame?\" asked Levin, and his life,\n",
      "and turning on its appulled atticul things.\n",
      " \"I'm sitting for you and hear your lips tonoy?\"\n",
      "asked Sergey Ivanovitch,\n",
      "washed, but it was not being caused in this spiritual comparisome,\n",
      "cheeking at the sight of her presenter\n",
      "interested had\n",
      "been comprehending angrily.\n",
      "\n",
      "\"No, he wanted to ask that!\"\n",
      "\n",
      "\"Told me\n",
      "is that\n",
      "me for the last moment.\"\n",
      "\n",
      "Levin lrowed agreements on his coan and then two regumed, and\n",
      "after dearing a steps in his father, and scared in with her\n",
      "strings of the hotliey one\n",
      "steps on her husband, who\n",
      "was an ited of his heart of his across where he always had\n",
      "fancied an\n",
      "tame. \"If these case, they less new sets-off illnassses is ig not so much as it must be that,\" said Se\n"
     ]
    }
   ],
   "source": [
    "print(sample(loaded, 2000, top_k = 10, prime=\"This is Geekquad \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
